{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.contrib.lookup import MutableHashTable\n",
    "from tensor2tensor.layers import common_layers\n",
    "\n",
    "from tensor2tensor.models import transformer\n",
    "sys.path.append('/workspace/MT/tensor2tensor/tensor2tensor/models/')\n",
    "\n",
    "from transformer_test import TransformerTest\n",
    "from transformer_cache import TransformerCache\n",
    "from tensor2tensor.data_generators import problem_hparams\n",
    "from transformer_cache import LRUCache\n",
    "from tensor2tensor.data_generators import problem\n",
    "\n",
    "sys.path.append('/workspace/MT/t2t_data_generators/')\n",
    "\n",
    "from generator import ShadENRUOpusProblem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found vocab file: /workspace/MT/shad_nlp18_contextNMT/data_4prev/vocab_enru.src\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-20 09:50:23,611] Found vocab file: /workspace/MT/shad_nlp18_contextNMT/data_4prev/vocab_enru.src\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found vocab file: /workspace/MT/shad_nlp18_contextNMT/data_4prev/vocab_enru.dst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-20 09:50:23,709] Found vocab file: /workspace/MT/shad_nlp18_contextNMT/data_4prev/vocab_enru.dst\n"
     ]
    }
   ],
   "source": [
    "shad_problem = ShadENRUOpusProblem()\n",
    "\n",
    "data = shad_problem.generate_encoded_samples(\n",
    "    '/workspace/MT/shad_nlp18_contextNMT/data_4prev/',\n",
    "    '/workspace/MT/shad_nlp18_contextNMT/data_4prev/',\n",
    "    problem.DatasetSplit.TEST\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_input = True\n",
    "mode=tf.estimator.ModeKeys.EVAL\n",
    "\n",
    "hparams = transformer.transformer_base_single_gpu()\n",
    "hparams.data_dir =  '/workspace/MT/shad_nlp18_contextNMT/data_4prev//'\n",
    "p_hparams = shad_problem.get_hparams(hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('batch_size_multiplier', 1), ('input_modality', {'inputs': ('symbol', 22057)}), ('input_space_id', 0), ('loss_multiplier', 1.0), ('stop_at_eos', 1), ('target_modality', ('symbol', 23180)), ('target_space_id', 0), ('vocabulary', {'inputs': <tensor2tensor.data_generators.text_encoder.SubwordTextEncoder object at 0x7f0b138a3cc0>, 'targets': <tensor2tensor.data_generators.text_encoder.SubwordTextEncoder object at 0x7f0b138a3d68>}), ('was_copy', False), ('was_reversed', False)]\n",
      "INFO:tensorflow:Unsetting shared_embedding_and_softmax_weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-20 09:50:24,079] Unsetting shared_embedding_and_softmax_weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting T2TModel mode to 'eval'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-20 09:50:24,081] Setting T2TModel mode to 'eval'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.dropout to 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-20 09:50:24,082] Setting hparams.dropout to 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-20 09:50:24,114] Setting hparams.relu_dropout to 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-20 09:50:24,115] Setting hparams.symbol_dropout to 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-20 09:50:24,116] Setting hparams.layer_prepostprocess_dropout to 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-20 09:50:24,117] Setting hparams.attention_dropout to 0.0\n"
     ]
    }
   ],
   "source": [
    "model = TransformerCache(hparams, mode, p_hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_true = {\n",
    "    \"inputs\": tf.placeholder(shape=[None, None, 1, 1], dtype=tf.int32, name=\"inputs\"),\n",
    "    \"targets\": tf.placeholder(shape=[None, None, 1, 1], dtype=tf.int32, name=\"inputs\"),\n",
    "    \"target_space_id\": tf.constant(1, dtype=tf.int32)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found vocab file: /workspace/MT/shad_nlp18_contextNMT/data_4prev/vocab_enru.src\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-20 09:50:24,150] Found vocab file: /workspace/MT/shad_nlp18_contextNMT/data_4prev/vocab_enru.src\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found vocab file: /workspace/MT/shad_nlp18_contextNMT/data_4prev/vocab_enru.dst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-20 09:50:24,242] Found vocab file: /workspace/MT/shad_nlp18_contextNMT/data_4prev/vocab_enru.dst\n"
     ]
    }
   ],
   "source": [
    "data = shad_problem.generate_encoded_samples(\n",
    "    '/workspace/MT/shad_nlp18_contextNMT/data_4prev/',\n",
    "    '/workspace/MT/shad_nlp18_contextNMT/data_4prev/',\n",
    "    problem.DatasetSplit.TEST\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_length = tf.placeholder(dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_result = model._greedy_infer(features_true, decode_length)[\"outputs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict = {\"/\".join([\"transformer\"] + var.name[:-2].split('/')[1:]) : var for var in tf.global_variables()[3:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(save_dict)\n",
    "ckpt = tf.train.get_checkpoint_state('/workspace/MT/train/transformer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /workspace/MT/train/transformer/model.ckpt-250000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-20 09:50:33,351] Restoring parameters from /workspace/MT/train/transformer/model.ckpt-250000\n"
     ]
    }
   ],
   "source": [
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    print(\"...no checkpoint found...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_saver = tf.train.Saver(tf.global_variables()[:2])\n",
    "new_ckpt = tf.train.get_checkpoint_state('/workspace/MT/train_cache/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /workspace/MT/train_cache/transformer_cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-20 09:50:33,832] Restoring parameters from /workspace/MT/train_cache/transformer_cache\n"
     ]
    }
   ],
   "source": [
    "if new_ckpt and new_ckpt.model_checkpoint_path:\n",
    "    new_saver.restore(sess, new_ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    print(\"...no checkpoint found...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = shad_problem.feature_encoders('/workspace/MT/shad_nlp18_contextNMT/data_4prev/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "flusher = 0\n",
    "with open('/workspace/MT/transformer_cache_test_ru.dst', 'w', encoding='utf8') as file:\n",
    "    for it in data:\n",
    "        inputs = np.array(it['inputs']).reshape((1, len(it['inputs']), 1, 1))\n",
    "        targets = np.array(it['targets']).reshape((1, len(it['targets']), 1, 1))\n",
    "        kek = sess.run([fast_result], feed_dict={\n",
    "            features_true['inputs'] : inputs,\n",
    "            features_true['targets'] : targets,\n",
    "            decode_length : len(targets)\n",
    "        })\n",
    "        string = encoders['targets'].decode(np.squeeze(kek[0])[:-1])\n",
    "        print(string, file=file)\n",
    "        counter += 1\n",
    "        flusher += 1\n",
    "        if flusher == 5:\n",
    "            model.sentence_cache.Flush()\n",
    "            flusher = 0\n",
    "        if counter == 5000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "with open('/workspace/MT/shad_nlp18_contextNMT/data_fused/en_test.src', 'r', encoding='utf8') as file:\n",
    "    with open('/workspace/MT/shad_nlp18_contextNMT/data_fused/en_test_short.src', 'w', encoding='utf8') as wfile:\n",
    "        for line in file:\n",
    "            print(line, file=wfile)\n",
    "            counter += 1\n",
    "            if counter == 5000:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "with open('/workspace/MT/shad_nlp18_contextNMT/data_fused/ru_test.dst', 'r', encoding='utf8') as file:\n",
    "    with open('/workspace/MT/shad_nlp18_contextNMT/data_fused/ru_test_short.dst', 'w', encoding='utf8') as wfile:\n",
    "        for line in file:\n",
    "            print(line, end='', file=wfile)\n",
    "            counter += 1\n",
    "            if counter == 5000:\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
